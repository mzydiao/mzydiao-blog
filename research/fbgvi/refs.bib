@InProceedings{pmlr-v202-diao23a,
  title = 	 {Forward-Backward {G}aussian Variational Inference via {JKO} in the Bures-{W}asserstein Space},
  author =       {Diao, Michael Ziyang and Balasubramanian, Krishna and Chewi, Sinho and Salim, Adil},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {7960--7991},
  year = 	 {2023},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/diao23a/diao23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/diao23a.html},
  abstract = 	 {Variational inference (VI) seeks to approximate a target distribution $\pi$ by an element of a tractable family of distributions. Of key interest in statistics and machine learning is Gaussian VI, which approximates $\pi$ by minimizing the Kullback-Leibler (KL) divergence to $\pi$ over the space of Gaussians. In this work, we develop the (Stochastic) Forward-Backward Gaussian Variational Inference (FB-GVI) algorithm to solve Gaussian VI. Our approach exploits the composite structure of the KL divergence, which can be written as the sum of a smooth term (the potential) and a non-smooth term (the entropy) over the Bures-Wasserstein (BW) space of Gaussians endowed with the Wasserstein distance. For our proposed algorithm, we obtain state-of-the-art convergence guarantees when $\pi$ is log-smooth and log-concave, as well as the first convergence guarantees to first-order stationary solutions when $\pi$ is only log-smooth.}
}

@misc{diao2023forwardbackward,
      title = 	 {Forward-Backward {G}aussian Variational Inference via {JKO} in the Bures-{W}asserstein Space},
      author={Michael Diao and Krishnakumar Balasubramanian and Sinho Chewi and Adil Salim},
      year={2023},
      eprint={2304.05398},
      archivePrefix={arXiv},
      primaryClass={math.ST}
}