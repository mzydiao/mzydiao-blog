---
title: "Forward-Backward Gaussian Variational Inference via JKO in the Bures-Wasserstein Space"
author: "Michael Diao"
date: "2023-07-23"
categories: [papers, ICML]
description: "We devise a novel algorithm for Gaussian variational inference that comes with state-of-the-art convergence guarantees."
abstract: Variational inference (VI) seeks to approximate a target distribution $\pi$ by an element of a tractable family of distributions. Of key interest in statistics and machine learning is Gaussian VI, which approximates $\pi$ by minimizing the Kullback-Leibler (KL) divergence to $\pi$ over the space of Gaussians. In this work, we develop the (Stochastic) Forward-Backward Gaussian Variational Inference (FB-GVI) algorithm to solve Gaussian VI. Our approach exploits the composite structure of the KL divergence, which can be written as the sum of a smooth term (the potential) and a non-smooth term (the entropy) over the Bures-Wasserstein (BW) space of Gaussians endowed with the Wasserstein distance. For our proposed algorithm, we obtain state-of-the-art convergence guarantees when $\pi$ is log-smooth and log-concave, as well as the first convergence guarantees to first-order stationary solutions when $\pi$ is only log-smooth.
bibliography: refs.bib
image: logistic.png
---

::: {.content-hidden}
$$
{{< include ../../_macros.tex >}}
$$
:::

# Links

Published at [ICML 2023](https://icml.cc/Conferences/2023).

For a quick overview, check out our ICML [poster](https://icml.cc/media/PosterPDFs/ICML%202023/23837.png) and [video](https://icml.cc/virtual/2023/poster/23837).

And for more detail, see our [arXiv preprint](https://arxiv.org/abs/2304.05398) [-@diao2023forwardbackward] and our [ICML paper](https://proceedings.mlr.press/v202/diao23a.html) [-@pmlr-v202-diao23a]. :)